from typing import Tuple, List, Dict, Union, Optional, Iterable, Pattern, Match
from pathlib import Path
from urllib.request import Request, urlopen
from pprint import pformat
from traceback import format_exception
from bs4 import BeautifulSoup, Tag
from datetime import timedelta
from logging import Logger, getLogger
import re
import json
import sys
import gzip
import time

from my_logger import MyLogger
from models.livedoor_news import LivedoorNews


class Crawler:
	"""Livedoorãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã§ã™ã€‚"""
	
	def __init__(self, logger: Optional[Logger] = None):
		self.__logger: Logger = logger if logger is not None else getLogger("crawl")
		self.__sources_directory_path: str = str(Path(__file__).parent)
		self.__json_directory_path: str = str(Path(self.__sources_directory_path).joinpath("../dataset/crawl"))
		self.__conf_directory_path: str = str(Path(self.__sources_directory_path).joinpath("../conf"))
		# TODO æœ¬å½“ã¯ãƒ•ã‚¡ã‚¤ãƒ«é–‹ã‘ãªã„å ´åˆã®ä¾‹å¤–å‡¦ç†ã‚‚å¿…è¦ï¼ˆä»¥ä¸‹ã™ã¹ã¦ã®openã«ãŠã„ã¦ï¼‰
		with open(self.__conf_directory_path + "/User-Agent.txt", mode = "r") as user_agent_conf:
			self.__user_agent: str = user_agent_conf.read( ).replace("\r\n", "").replace("\n", "")
		self.__length: int = 0
		self.__count: int = 0
		self.__error_count: int = 0
		self.__delete_count: int = 0
		self.__critical_count: int = 0
		self.__start_time: float = 0.0
		self.__file_name: str = ""
	
	def main(self):
		# TODO ä»¥ä¸‹3è¡Œï¼Œä¾‹å¤–å‡¦ç†ã—ã¦ã„ãªã„ãŸã‚ï¼Œåˆ¥ã®å…¥åŠ›ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹
		file_name: str = input("ã©ã‚Œã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ï¼Ÿï¼ˆdebug / develop / test / trainï¼‰ï¼š")
		range_from: int = int(input("é–‹å§‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ï¼Ÿï¼ˆ0ã§æŒ‡å®šãªã—ï¼‰ï¼š"))
		range_to: int = int(input("çµ‚äº†ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ï¼Ÿï¼ˆ0ã§æŒ‡å®šãªã—ï¼‰ï¼š"))
		self.__file_name = file_name
		self.__crawl(file_name, range_from, range_to)
	
	def __crawl(self, file_name: str, range_from: int, range_to: int):
		"""ã²ã¨ã¤ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚\n
		:param file_name: JSONã®ãƒ•ã‚¡ã‚¤ãƒ«å"""
		self.__logger.info(file_name + " ã® " + str(range_from) + " ã‹ã‚‰ " + str(range_to) + " ã¾ã§ã‚’å–å¾—ã—ã¾ã™ã€‚")
		before_data: Tuple[LivedoorNews, ...] = self.__get_data(file_name, range_from, range_to)
		self.__length = len(before_data)
		if self.__length < 1:
			self.__logger.error("ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
			return
		self.__logger.info("å–å¾—ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ•°ï¼š" + str(self.__length))
		self.__start_time = time.time( )
		tuple(map(self.__crawl_one, before_data))
		self.__logger.info("ã€å®Œäº†ã€‘")
		self.__disp_progress( )
	
	def __get_data(self, file_name: str, range_from: int, range_to: int) -> Tuple[LivedoorNews, ...]:
		"""å…¥åŠ›ã•ã‚ŒãŸæƒ…å ±ã‚’åŸºã«ï¼ŒJSONã‹ã‚‰LivedoorNewsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n
		:param file_name: JSONã®è¡¨ç¤ºå
		:param range_from: JSONã®ä½•ç•ªç›®ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã‚‰å–å¾—ã™ã‚‹ã‹ï¼ˆ0ã§æœ€åˆã‹ã‚‰ï¼‰
		:param range_to: JSONã®ä½•ç•ªç›®ã¾ã§ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã™ã‚‹ã‹ï¼ˆ0ã§æœ€å¾Œã¾ã§ï¼‰
		:return: ç”Ÿæˆã—ãŸLivedoorNewsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®çµ„"""
		with open(self.__json_directory_path + "/" + file_name + ".json", mode = "r") as json_file:
			data_json: List[Dict[str, Union[int, Optional[bool], str]]] = json.load(json_file)
		all_data: Tuple[LivedoorNews, ...] = tuple(map(lambda dic: LivedoorNews(dic.get("year"), dic.get("month"), dic.get("category"), dic.get("id"), dic.get("is_series")), data_json))
		if range_to == 0:
			range_to = len(all_data)
		return tuple(map(lambda tup: tup[1], filter(lambda tup: range_from <= tup[0] and tup[0] < range_to, enumerate(all_data))))
	
	def __crawl_one(self, news: LivedoorNews):
		"""ã²ã¨ã¤ã®LivedoorNewsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ã¤ã„ã¦ï¼Œã‚¯ãƒ­ãƒ¼ãƒ«ã‚’å®Ÿæ–½ã—ï¼Œã‚¿ã‚¤ãƒˆãƒ«ï¼Œè¦ç´„ï¼Œè¨˜äº‹å†…å®¹ã‚’è¿½åŠ ã—ã¾ã™ã€‚\n
		:param news: å‡¦ç†ã™ã‚‹LivedoorNewsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
		:return: å…¥åŠ›ã¨åŒã˜ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"""
		self.__disp_progress( )
		self.__count += 1
		html: str = self.__id_to_html(news.id)
		if html == "":
			self.__sleep( )
			return
		article: Dict[str, str] = self.__parse_html(html, news.id)
		if article.get("error") is not None:
			self.__sleep( )
			return
		self.__update_json(news, article)
		self.__sleep( )
		return
	
	def __disp_progress(self):
		"""é€²æ—ã‚’ãƒ­ã‚°å‡ºåŠ›ã—ã¾ã™ã€‚"""
		now_time: float = time.time( )
		self.__logger.info("\nâˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’\né€²æ—ï¼š" + str(self.__count) + " / " + str(self.__length) + "ï¼ˆ" + str(round(float(self.__count) / float(self.__length) * 100.0, 2)) + " ï¼…ï¼‰ï¼Œã‚¨ãƒ©ãƒ¼æ•°ï¼š" + str(self.__error_count) + "å›ï¼ˆ" +str(round(float(self.__error_count) / (float(self.__count) + 0.00001) * 100.0, 2)) + " ï¼…ï¼‰ï¼Œé‡å¤§ã‚¨ãƒ©ãƒ¼æ•°ï¼š" + str(self.__critical_count) + "å›ï¼ˆ" +str(round(float(self.__critical_count) / (float(self.__count) + 0.00001) * 100.0, 2)) + " ï¼…ï¼‰\nå‰Šé™¤æ¸ˆã¿æ•°ï¼š" + str(self.__delete_count) + "ä»¶ï¼ˆ" +str(round(float(self.__delete_count) / (float(self.__count) + 0.00001) * 100.0, 2)) + " ï¼…ï¼‰\nçµŒéæ™‚é–“ï¼š" + str(timedelta(seconds = (now_time - self.__start_time))) + "ï¼Œæ¨å®šæ®‹ã‚Šæ™‚é–“ï¼š" + str(timedelta(seconds = (now_time - self.__start_time) * (float(self.__length) / (float(self.__count) + 0.00001)) - (now_time - self.__start_time))))
	
	def __sleep(self):
		"""ã‚¯ãƒ­ãƒ¼ãƒ«é–“éš”ã‚’ç©ºã‘ã‚‹ãŸã‚ï¼Œã„ã£ãŸã‚“10ç§’ãŠã‚„ã™ã¿ã—ã¾ã™ã€‚"""
		self.__logger.debug("ã„ã£ãŸã‚“ ãŠã‚„ã™ã¿ ğŸŒ™ ğŸ’¤")
		time.sleep(10.0)
	
	def __id_to_html(self, id: int) -> str:
		"""idã‹ã‚‰HTMLã‚’å–å¾—ã—ã¾ã™ã€‚"""
		url: str = "https://news.livedoor.com/article/detail/" + str(id) + "/"
		self.__logger.debug("IDâ†’HTMLï¼š" + url)
		headers: Dict[str, str] = {
			"User-Agent": self.__user_agent,
			"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
			"Accept-Language": "ja,en;q=0.5",
			"Accept-Encoding": "gzip",
			"Connection": "keep-alive",
			"Cookie": "",
			"Upgrade-Insecure-Requests": "1",
			"Pragma": "no-cache",
			"Cache-Control": "no-cache"
		}
		http_request: Request = Request(url, headers = headers)
		try:
			with urlopen(http_request) as http_response:
				response_headers: Dict[str, str] = http_response.info( )
				comp_type: str = response_headers["Content-Encoding"] if "Content-Encoding" in response_headers else "gzip"
				content_type: str = response_headers["Content-Type"] if "Content-Type" in response_headers else "text/html; charset=utf-8"
				encoding = content_type[content_type.find("charset=") + 8: ]
				response_byte: bytes = http_response.read( )
				if comp_type == "gzip":
					response_byte = gzip.decompress(response_byte)
				return response_byte.decode(encoding = encoding, errors = "ignore")
		except Exception as exception:
			self.__logger.exception(str(id) + " ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
			self.__error_count += 1
			return ""
	
	def __parse_html(self, html: str, id: int) -> Dict[str, str]:
		"""HTMLã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ï¼Œè¦ç´„ï¼Œæœ¬æ–‡ã‚’æŠ½å‡ºã—ã¾ã™ã€‚\n
		:param html: HTMLå½¢å¼ã®æ–‡å­—åˆ—
		:param id: è¨˜äº‹ID
		:return: { 'title': '(title)', 'summary': '(summary)', 'content': '(content)' }ï¼Œã‚¨ãƒ©ãƒ¼ã®å ´åˆ{ 'error': '(error)' }"""
		self.__logger.debug("HTMLè§£æ")
		beautiful_soup: BeautifulSoup = BeautifulSoup(markup = html, features = "html5lib")
		try:
			# ã€Œã€‚ã€åŒºåˆ‡ã‚Šã§ã²ã¨ã¾ã¨ã¾ã‚Šã«ãªã£ã¦ã„ã‚‹ãŸã‚metaã‚¿ã‚°ã‹ã‚‰è¦ç´„ã‚’å–å¾—
			meta_discription: Tag = beautiful_soup.find(name = "meta", attrs = {"name": "description"})
			summary: str = meta_discription.get("content") + "ã€‚"
			# ä»˜åŠ æƒ…å ±ãŒä»˜ã„ã¦ã„ãªã„ãŸã‚metaã‚¿ã‚°ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
			meta_ob_title: Tag = beautiful_soup.find(name = "meta", attrs = {"property": "ob:title"})
			title: str = meta_ob_title.get("content")
		except Exception as exception:
			self.__logger.exception(str(id) + " ã‚’è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚")
			self.__error_count += 1
			return {"error": "error"}
		try:
			# æœ¬æ–‡ã‚’å–å¾—ï¼ˆã‚µã‚¤ãƒˆã®æœ¬æ–‡å‰å¾Œã®ã‚¿ã‚°ã«é–“é•ã„ãŒã‚ã‚‹ãŸã‚æ­£ã—ãå–ã‚Œã‚‹ä¿è¨¼ã¯ãªã„ï¼‰
			span_article: Tag = beautiful_soup.find(name = "div", attrs = {"class": "articleBody"}).find(name = "span", attrs = {"itemprop": "articleBody"})
			tuple(map(lambda script: script.decompose( ), span_article.find_all(name = "script")))
			content: str = span_article.get_text(separator = "", strip = True).replace("\r\n", "").replace("\n", "")
		except Exception as exception:
			self.__logger.error("ã€å‰Šé™¤ã€‘" + str(id) + " ã¯ã™ã§ã«å‰Šé™¤ã•ã‚Œã¦ã„ã¾ã™ã€‚")
			self.__error_count += 1
			self.__delete_count += 1
			return {"error": "error"}
		return self.__check_html(title, summary, content)
	
	def __check_html(self, title: str, summary: str, content: str) -> Dict[str, str]:
		"""HTMLè§£æçµæœãŒæ­£ã—ã‹ã£ãŸã‹ç¢ºèªã—ã¾ã™ã€‚\n
		:param title: HTMLã‹ã‚‰å–å¾—ã—ãŸã‚¿ã‚¤ãƒˆãƒ«
		:param summary: HTMLã‹ã‚‰å–å¾—ã—ãŸè¦ç´„
		:param content: HTMLã‹ã‚‰å–å¾—ã—ãŸæœ¬æ–‡
		return: { 'title': '(title)', 'summary': '(summary)', 'content': '(content)' }ï¼Œã‚¨ãƒ©ãƒ¼ã®å ´åˆ{ 'error': '(error)' }"""
		is_title: bool = 1 < len(title)
		summary_pattern: Pattern = re.compile("[\\W\\w]{1,}?ã€‚[\\W\\w]{1,}?ã€‚[\\W\\w]{1,}?ã€‚")
		summary_match: Match = summary_pattern.fullmatch(summary)
		is_summary: bool = summary_match is not None and 1 < len(summary_match.group(0))
		is_content: bool = 1 < len(content)
		if not (is_title and is_summary and is_content):
			self.__error_count += 1
			self.__logger.error("HTMLã®è§£æçµæœãŒæ­£ã—ããªã„ã‚ˆã†ã§ã™ã€‚")
			return {"error": "error"}
		return {
			"title": title,
			"summary": summary,
			"content": content
		}
	
	def __update_json(self, news: LivedoorNews, article: Dict[str, str]):
		"""å–å¾—ã—ãŸè¨˜äº‹æƒ…å ±ã‚’å…ƒã«JSONã‚’æ›´æ–°ã—ã¾ã™"""
		self.__logger.debug("JSONæ›¸ãè¾¼ã¿")
		news.title = article.get("title")
		news.summary = article.get("summary")
		news.content = article.get("content")
		write_data: dict = news.dict_like_json
		try:
			json_string: str = json.dumps(write_data, ensure_ascii = False, allow_nan = False, indent = "\t").replace("\n", "\r\n") + "\r\n"
			with open(self.__json_directory_path + "/" + self.__file_name + "/" + str(news.id) + ".json", mode = "w") as json_write_file:
				json_write_file.write(json_string)
		except Exception as exception:
			self.__error_count += 1
			self.__critical_count += 1
			self.__logger.error("ã€é‡å¤§ã€‘" + self.__file_name + " JSONãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚\nIDï¼š" + str(id) + "\nå‡¦ç†ç•ªå·ï¼ˆcountï¼‰ï¼š" + str(self.__count) + "\næ›´æ–°ãƒ‡ãƒ¼ã‚¿ï¼š" + json.dumps(news.dict_like_json, ensure_ascii = False, allow_nan = False, indent = "\t").replace("\n", "\r\n"))
			return


if __name__ == "__main__":
	logger: Logger = MyLogger("crawl").logger
	Crawler(logger).main( )
